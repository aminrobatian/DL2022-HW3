{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_6DC1Vm2iQm"
   },
   "source": [
    "# Deep Learning\n",
    "## HW3 - Problem 2\n",
    "\n",
    "Name: Amin Robatian\n",
    "\n",
    "Student Number: 400301075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets, transforms, ops\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchsummary import summary\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from math import floor\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(85)\n",
    "random.seed(85)\n",
    "np.random.seed(85)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdtwqGa3tFod"
   },
   "source": [
    "# Part (A) - ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_ResNet50 = torchvision.models.resnet50(pretrained=True)\n",
    "print(model_ResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Trainable params: 25,557,032\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model_ResNet50.parameters() if p.requires_grad)\n",
    "print(f\"Number of Trainable params: {pytorch_total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Trainable params: 0\n"
     ]
    }
   ],
   "source": [
    "for param in model_ResNet50.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model_ResNet50.parameters() if p.requires_grad)\n",
    "print(f\"Number of Trainable params: {pytorch_total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Trainable params: 20,490\n"
     ]
    }
   ],
   "source": [
    "model_ResNet50.fc = nn.Sequential(nn.Linear(2048, 10))\n",
    "\n",
    "model_ResNet50 = model_ResNet50.to(device)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model_ResNet50.parameters() if p.requires_grad)\n",
    "print(f\"Number of Trainable params: {pytorch_total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "50,000 Training Images\n",
      "Files already downloaded and verified\n",
      "10,000 Test Images\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "target_transform = Lambda(lambda y: torch.zeros(10, \n",
    "                                                dtype=torch.float).scatter_(dim=0, index=torch.tensor(y),value=1))\n",
    "\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "print(f\"{len(train_dataset):,} Training Images\")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "print(f\"{len(test_dataset):,} Test Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainDataSet_Accuracy(dataloader, model, loss_fn):\n",
    "    size = floor(len(dataloader.dataset) / batch_size) * batch_size\n",
    "    num_batches = floor(len(dataloader.dataset) / batch_size)\n",
    "    train_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            if len(X) < batch_size:\n",
    "              break\n",
    "            pred = model(X)\n",
    "            train_loss += loss_fn(pred, y).item()\n",
    "            #\n",
    "            for i in range(batch_size):\n",
    "              v1 = pred[i]\n",
    "              v2 = y[i]\n",
    "              if torch.argmax(v1) == torch.argmax(v2):\n",
    "                correct += 1\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    train_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Train Dataset: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
    "    accuracy = 100*correct\n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestDataSet_Accuracy(dataloader, model, loss_fn):\n",
    "    size = floor(len(dataloader.dataset) / batch_size) * batch_size\n",
    "    num_batches = floor(len(dataloader.dataset) / batch_size)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            if len(X) < batch_size:\n",
    "              break\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            #\n",
    "            for i in range(batch_size):\n",
    "              v1 = pred[i]\n",
    "              v2 = y[i]\n",
    "              if torch.argmax(v1) == torch.argmax(v2):\n",
    "                correct += 1\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Dataset: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    accuracy = 100*correct\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 79.3%, Avg loss: 0.630236 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 78.4%, Avg loss: 0.640711 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 80.4%, Avg loss: 0.578347 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 79.7%, Avg loss: 0.596580 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 81.3%, Avg loss: 0.548008 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 80.4%, Avg loss: 0.569842 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 81.6%, Avg loss: 0.536568 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 80.7%, Avg loss: 0.557460 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 82.2%, Avg loss: 0.524954 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 81.0%, Avg loss: 0.552790 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 82.6%, Avg loss: 0.506352 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 81.6%, Avg loss: 0.536827 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 82.9%, Avg loss: 0.499738 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 81.5%, Avg loss: 0.533979 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 82.7%, Avg loss: 0.505349 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 81.3%, Avg loss: 0.540465 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 83.3%, Avg loss: 0.488238 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 82.0%, Avg loss: 0.525376 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 83.5%, Avg loss: 0.482460 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 81.9%, Avg loss: 0.520792 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_ResNet50.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "train_loss = np.zeros(epochs)\n",
    "train_accuracy = np.zeros(epochs)\n",
    "test_loss = np.zeros(epochs)\n",
    "test_accuracy = np.zeros(epochs)\n",
    "\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    model_ResNet50.train()\n",
    "    train_loop(train_dataloader, model_ResNet50, loss_fn, optimizer)\n",
    "    model_ResNet50.eval()\n",
    "    train_loss[t], train_accuracy[t] = TrainDataSet_Accuracy(train_dataloader, model_ResNet50, loss_fn)\n",
    "    test_loss[t], test_accuracy[t] = TestDataSet_Accuracy(test_dataloader, model_ResNet50, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Trainable params: 0\n"
     ]
    }
   ],
   "source": [
    "for param in model_ResNet50.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model_ResNet50.parameters() if p.requires_grad)\n",
    "print(f\"Number of Trainable params: {pytorch_total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUJsEUuztJKx"
   },
   "source": [
    "# Part (B) - Teacher: ResNet50, Student: ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_ResNet18 = torchvision.models.resnet18()\n",
    "print(model_ResNet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Trainable params: 11,181,642\n"
     ]
    }
   ],
   "source": [
    "model_ResNet18.fc = nn.Sequential(nn.Linear(512, 10))\n",
    "\n",
    "model_ResNet18 = model_ResNet18.to(device)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model_ResNet18.parameters() if p.requires_grad)\n",
    "print(f\"Number of Trainable params: {pytorch_total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropy(y_hat, y):\n",
    "  CE =  (-1 / batch_size) * torch.sum(y * torch.log(y_hat) + (1 - y) * torch.log(1 - y_hat))\n",
    "  return CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_loss(logit_teacher, logit_student, y):\n",
    "    m = nn.Softmax(dim=1)\n",
    "    alpha = 0.5\n",
    "    T = 3\n",
    "    loss =  (1 - alpha) * CrossEntropy(m(logit_student / T), y) \\\n",
    "    + alpha * CrossEntropy(m(logit_student / T), m(logit_teacher / T)) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model_teacher, model_student, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    " \n",
    "        logit_teacher = model_teacher(X)\n",
    "        logit_student = model_student(X)\n",
    "\n",
    "        loss = loss_fn(logit_teacher, logit_student, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainDataSet_Accuracy(dataloader, model_teacher, model_student, loss_fn):\n",
    "    size = floor(len(dataloader.dataset) / batch_size) * batch_size\n",
    "    num_batches = floor(len(dataloader.dataset) / batch_size)\n",
    "    train_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            logit_teacher = model_teacher(X)\n",
    "            logit_student = model_student(X)\n",
    "            \n",
    "            if len(X) < batch_size:\n",
    "              break\n",
    "            pred = model_student(X)\n",
    "            train_loss += loss_fn(logit_teacher, logit_student, y).item()\n",
    "            #\n",
    "            for i in range(batch_size):\n",
    "              v1 = pred[i]\n",
    "              v2 = y[i]\n",
    "              if torch.argmax(v1) == torch.argmax(v2):\n",
    "                correct += 1\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    train_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Train Dataset: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
    "    accuracy = 100*correct\n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestDataSet_Accuracy(dataloader, model_teacher, model_student, loss_fn):\n",
    "    size = floor(len(dataloader.dataset) / batch_size) * batch_size\n",
    "    num_batches = floor(len(dataloader.dataset) / batch_size)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            logit_teacher = model_teacher(X)\n",
    "            logit_student = model_student(X)\n",
    "        \n",
    "            if len(X) < batch_size:\n",
    "              break\n",
    "            pred = model_student(X)\n",
    "            test_loss += loss_fn(logit_teacher, logit_student, y).item()\n",
    "            #\n",
    "            for i in range(batch_size):\n",
    "              v1 = pred[i]\n",
    "              v2 = y[i]\n",
    "              if torch.argmax(v1) == torch.argmax(v2):\n",
    "                correct += 1\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Dataset: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    accuracy = 100*correct\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 44.3%, Avg loss: 2.658878 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 44.6%, Avg loss: 2.658935 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 54.2%, Avg loss: 2.497886 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 52.9%, Avg loss: 2.514237 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 42.7%, Avg loss: 2.943191 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 42.6%, Avg loss: 2.965683 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 64.7%, Avg loss: 2.303910 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 62.2%, Avg loss: 2.342232 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 70.4%, Avg loss: 2.212764 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 67.2%, Avg loss: 2.258058 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 61.4%, Avg loss: 2.408392 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 58.4%, Avg loss: 2.459478 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 77.0%, Avg loss: 2.111169 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 72.5%, Avg loss: 2.184384 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 80.3%, Avg loss: 2.062280 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 74.1%, Avg loss: 2.154864 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 83.5%, Avg loss: 2.018980 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 75.9%, Avg loss: 2.129831 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 84.8%, Avg loss: 2.001428 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 75.8%, Avg loss: 2.133971 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = distillation_loss\n",
    "optimizer = torch.optim.SGD(model_ResNet18.parameters(), lr=learning_rate, momentum=0.9)\n",
    "model_ResNet50.eval()\n",
    "\n",
    "train_loss = np.zeros(epochs)\n",
    "train_accuracy = np.zeros(epochs)\n",
    "test_loss = np.zeros(epochs)\n",
    "test_accuracy = np.zeros(epochs)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    model_ResNet18.train()\n",
    "    train_loop(train_dataloader,model_ResNet50, model_ResNet18, loss_fn, optimizer)\n",
    "    model_ResNet18.eval()\n",
    "    train_loss[t], train_accuracy[t] = TrainDataSet_Accuracy(train_dataloader, model_ResNet50, model_ResNet18, loss_fn)\n",
    "    test_loss[t], test_accuracy[t] = TestDataSet_Accuracy(test_dataloader, model_ResNet50, model_ResNet18, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzBdV9YotKrJ"
   },
   "source": [
    "# Part (C) - ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Trainable params: 11,181,642\n"
     ]
    }
   ],
   "source": [
    "model_ResNet18 = torchvision.models.resnet18()\n",
    "\n",
    "model_ResNet18.fc = nn.Sequential(nn.Linear(512, 10))\n",
    "\n",
    "model_ResNet18 = model_ResNet18.to(device)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model_ResNet18.parameters() if p.requires_grad)\n",
    "print(f\"Number of Trainable params: {pytorch_total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainDataSet_Accuracy(dataloader, model, loss_fn):\n",
    "    size = floor(len(dataloader.dataset) / batch_size) * batch_size\n",
    "    num_batches = floor(len(dataloader.dataset) / batch_size)\n",
    "    train_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            if len(X) < batch_size:\n",
    "              break\n",
    "            pred = model(X)\n",
    "            train_loss += loss_fn(pred, y).item()\n",
    "            #\n",
    "            for i in range(batch_size):\n",
    "              v1 = pred[i]\n",
    "              v2 = y[i]\n",
    "              if torch.argmax(v1) == torch.argmax(v2):\n",
    "                correct += 1\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    train_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Train Dataset: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
    "    accuracy = 100*correct\n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestDataSet_Accuracy(dataloader, model, loss_fn):\n",
    "    size = floor(len(dataloader.dataset) / batch_size) * batch_size\n",
    "    num_batches = floor(len(dataloader.dataset) / batch_size)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            if len(X) < batch_size:\n",
    "              break\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            #\n",
    "            for i in range(batch_size):\n",
    "              v1 = pred[i]\n",
    "              v2 = y[i]\n",
    "              if torch.argmax(v1) == torch.argmax(v2):\n",
    "                correct += 1\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Dataset: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    accuracy = 100*correct\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 52.1%, Avg loss: 1.324067 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 51.1%, Avg loss: 1.340711 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 57.3%, Avg loss: 1.198751 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 55.3%, Avg loss: 1.270733 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 69.1%, Avg loss: 0.863157 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 65.2%, Avg loss: 0.977534 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 76.5%, Avg loss: 0.665944 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 70.4%, Avg loss: 0.833737 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 80.3%, Avg loss: 0.561722 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 72.3%, Avg loss: 0.797561 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 83.3%, Avg loss: 0.474058 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 73.3%, Avg loss: 0.765521 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 89.8%, Avg loss: 0.304579 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 75.8%, Avg loss: 0.714778 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 92.9%, Avg loss: 0.215563 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 75.7%, Avg loss: 0.759521 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 90.1%, Avg loss: 0.280666 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 72.4%, Avg loss: 0.978519 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 95.0%, Avg loss: 0.146129 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 74.4%, Avg loss: 0.867977 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_ResNet18.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "train_loss = np.zeros(epochs)\n",
    "train_accuracy = np.zeros(epochs)\n",
    "test_loss = np.zeros(epochs)\n",
    "test_accuracy = np.zeros(epochs)\n",
    "\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    model_ResNet18.train()\n",
    "    train_loop(train_dataloader, model_ResNet18, loss_fn, optimizer)\n",
    "    model_ResNet18.eval()\n",
    "    train_loss[t], train_accuracy[t] = TrainDataSet_Accuracy(train_dataloader, model_ResNet18, loss_fn)\n",
    "    test_loss[t], test_accuracy[t] = TestDataSet_Accuracy(test_dataloader, model_ResNet18, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Result:**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*   As the model becomes smaller, it becomes difficult for the model to learn many complex properties of the data, so the accuracy of the model on the test data decreases.\n",
    "*    In general, the use of the teacher model helps the accuracy of the student model and its training, but considering that in our problem, the accuracy of the teacher model is about the accuracy of the student model on the new dataset, using Knowledge Distillation technique has not helped us much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7BpkNihtM3w"
   },
   "source": [
    "# Part (D) - ResNet50 Fine Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Trainable params: 23,528,522\n"
     ]
    }
   ],
   "source": [
    "model_ResNet50 = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "model_ResNet50.fc = nn.Sequential(nn.Linear(2048, 10))\n",
    "\n",
    "model_ResNet50 = model_ResNet50.to(device)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model_ResNet50.parameters() if p.requires_grad)\n",
    "print(f\"Number of Trainable params: {pytorch_total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 97.6%, Avg loss: 0.079643 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 95.3%, Avg loss: 0.139978 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 99.2%, Avg loss: 0.029195 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 95.9%, Avg loss: 0.129463 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 99.6%, Avg loss: 0.014123 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 96.1%, Avg loss: 0.127551 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 99.9%, Avg loss: 0.006038 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 96.1%, Avg loss: 0.124864 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 100.0%, Avg loss: 0.003117 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 96.4%, Avg loss: 0.121331 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 99.9%, Avg loss: 0.003166 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 96.3%, Avg loss: 0.121296 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 100.0%, Avg loss: 0.001476 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 96.4%, Avg loss: 0.132235 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 100.0%, Avg loss: 0.001022 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 96.6%, Avg loss: 0.121314 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 100.0%, Avg loss: 0.000504 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 96.7%, Avg loss: 0.122805 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train Dataset: \n",
      " Accuracy: 100.0%, Avg loss: 0.000842 \n",
      "\n",
      "Test Dataset: \n",
      " Accuracy: 96.7%, Avg loss: 0.127451 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_ResNet50.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "train_loss = np.zeros(epochs)\n",
    "train_accuracy = np.zeros(epochs)\n",
    "test_loss = np.zeros(epochs)\n",
    "test_accuracy = np.zeros(epochs)\n",
    "\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    model_ResNet50.train()\n",
    "    train_loop(train_dataloader, model_ResNet50, loss_fn, optimizer)\n",
    "    model_ResNet50.eval()\n",
    "    train_loss[t], train_accuracy[t] = TrainDataSet_Accuracy(train_dataloader, model_ResNet50, loss_fn)\n",
    "    test_loss[t], test_accuracy[t] = TestDataSet_Accuracy(test_dataloader, model_ResNet50, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Result:**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*   ResNet50 is a large model that can easily learn the complex features of the dataset and since we train the model from the scratch, it is trained well on CIFAR-10 dataset. \n",
    "But due to the large size of the network, training is very time-consuming and the amount of calculations is very huge.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
